{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e2a030e3",
      "metadata": {
        "id": "e2a030e3"
      },
      "source": [
        "# Advanced Machine Learning using Spark MLlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d9ae225b",
      "metadata": {
        "id": "d9ae225b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b006cd7e-7c0a-4a61-a2c1-fa17383c58d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.9999999999999992]\n",
            "Intercept: 15.000000000000009\n"
          ]
        }
      ],
      "source": [
        "# Example: Linear Regression with Spark MLlib\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
        "\n",
        "# Load sample data\n",
        "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
        "columns = ['ID', 'Feature', 'Target']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Prepare data for modeling\n",
        "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Train a linear regression model\n",
        "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
        "model = lr.fit(df_transformed)\n",
        "\n",
        "# Print model coefficients\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0b266267",
      "metadata": {
        "id": "0b266267",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c48531fc-dee8-4e65-b014-32bb676bda70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a simple dataset\n",
        "data = [\n",
        "    (0, 1.0, 0.1),\n",
        "    (1, 2.0, 1.1),\n",
        "    (2, 3.0, 1.5),\n",
        "    (3, 4.0, 1.7),\n",
        "    (4, 5.0, 1.8),\n",
        "]\n",
        "columns = ['ID', 'Feature1', 'Feature2']\n",
        "\n",
        "# Create a DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Combine features into a single vector column (required for MLlib)\n",
        "assembler = VectorAssembler(inputCols=['Feature1', 'Feature2'], outputCol='features')\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Define the target variable (label)\n",
        "# For simplicity, let's use a rule to generate labels: 1 if Feature1 > 2, otherwise 0\n",
        "df = df.withColumn(\"Label\", (col(\"Feature1\") > 2).cast(\"int\"))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Create the logistic regression model\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='Label')\n",
        "\n",
        "# Train the model on the training data\n",
        "model = lr.fit(train_data)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Label')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b9066e04",
      "metadata": {
        "id": "b9066e04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22dc4a7e-773b-4872-a080-40f70f588dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Within Set Sum of Squared Errors (WSSSE): 8.75\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Step 1: Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"KMeansExample\").getOrCreate()\n",
        "\n",
        "# Step 2: Create a simple dataset (features: Feature1 and Feature2)\n",
        "data = [\n",
        "    (1, 1.0, 1.1),\n",
        "    (2, 1.5, 1.6),\n",
        "    (3, 3.0, 3.1),\n",
        "    (4, 3.5, 3.6),\n",
        "    (5, 8.0, 8.1),\n",
        "    (6, 8.5, 8.6),\n",
        "]\n",
        "columns = ['ID', 'Feature1', 'Feature2']\n",
        "\n",
        "# Create a DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Combine features into a single vector column (required for MLlib)\n",
        "assembler = VectorAssembler(inputCols=['Feature1', 'Feature2'], outputCol='features')\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Create the KMeans model\n",
        "kmeans = KMeans().setK(2).setSeed(1)  # K=2 means we want 2 clusters\n",
        "\n",
        "# Train the KMeans model\n",
        "model = kmeans.fit(df)\n",
        "\n",
        "# Make predictions (assign data points to clusters)\n",
        "predictions = model.transform(df)\n",
        "\n",
        "# Evaluate the model (Within Set Sum of Squared Errors)\n",
        "wssse = model.summary.trainingCost\n",
        "print(f\"Within Set Sum of Squared Errors (WSSSE): {wssse}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# Step 1: Start a Spark session\n",
        "spark = SparkSession.builder.appName(\"LinearRegressionCV\").getOrCreate()\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "data_path = \"car_incident_canada (1).csv\"\n",
        "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "\n",
        "# Step 3: Data preprocessing\n",
        "# Identify feature columns and label column\n",
        "feature_columns = [col for col in df.columns if col != \"target\"]  # Replace \"target\" with the actual label column name\n",
        "label_column = \"C_SEV\"  # Replace \"target\" with the actual label column name\n",
        "\n",
        "# Assemble features into a single vector\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "data = assembler.transform(df).select(\"features\", label_column)\n",
        "\n",
        "# Step 4: Split the data into training and test sets\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Step 5: Define the model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=label_column)\n",
        "\n",
        "# Step 6: Hyperparameter tuning\n",
        "paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(lr.regParam, [0.01, 0.1, 0.5])  # Regularization parameter\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])  # ElasticNet mixing parameter\n",
        "    .build()\n",
        ")\n",
        "\n",
        "# Define cross-validator\n",
        "cv = CrossValidator(\n",
        "    estimator=lr,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=RegressionEvaluator(labelCol=label_column, metricName=\"rmse\"),\n",
        "    numFolds=10\n",
        ")\n",
        "\n",
        "# Step 7: Train the model with cross-validation\n",
        "cv_model = cv.fit(train_data)\n",
        "\n",
        "# Step 8: Evaluate the model on the test set\n",
        "test_predictions = cv_model.transform(test_data)\n",
        "evaluator = RegressionEvaluator(labelCol=label_column, metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(test_predictions)\n",
        "\n",
        "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse}\")\n",
        "\n",
        "# Get the best model and print its parameters\n",
        "best_model = cv_model.bestModel\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"  regParam: {best_model._java_obj.getRegParam()}\")\n",
        "print(f\"  elasticNetParam: {best_model._java_obj.getElasticNetParam()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds6mhrwUX9S1",
        "outputId": "7558ca8d-67a3-4b79-c9cf-d078bebb7ca0"
      },
      "id": "Ds6mhrwUX9S1",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE) on test data: 0.009903192314947344\n",
            "Best Model Parameters:\n",
            "  regParam: 0.01\n",
            "  elasticNetParam: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YWFyYvCAasor"
      },
      "id": "YWFyYvCAasor",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}